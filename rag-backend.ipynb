{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ RAG Pipeline - Kaggle Backend with Ngrok\n",
    "\n",
    "This notebook sets up a complete RAG backend with FastAPI and exposes it via ngrok.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Enable **Internet** in Kaggle notebook settings\n",
    "2. Get your ngrok auth token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "\n",
    "**Run cells in order!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:02:51.002855Z",
     "iopub.status.busy": "2026-02-03T08:02:51.002227Z",
     "iopub.status.idle": "2026-02-03T08:03:30.820026Z",
     "shell.execute_reply": "2026-02-03T08:03:30.817686Z",
     "shell.execute_reply.started": "2026-02-03T08:02:51.002753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (26.0rc2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "‚úÖ All dependencies installed!\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Install Dependencies\n",
    "!pip install fastapi uvicorn pyngrok python-multipart --quiet\n",
    "!pip install torch transformers faiss-cpu rank_bm25 rouge_score sentence-transformers PyPDF2 --quiet\n",
    "!pip install scikit-learn psutil nltk pydantic --quiet\n",
    "!pip install torch transformers faiss-cpu rank_bm25 rouge_score sentence-transformers PyPDF2 --quiet\n",
    "!pip install spacy\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:30.825629Z",
     "iopub.status.busy": "2026-02-03T08:03:30.824159Z",
     "iopub.status.idle": "2026-02-03T08:03:31.002714Z",
     "shell.execute_reply": "2026-02-03T08:03:31.001321Z",
     "shell.execute_reply.started": "2026-02-03T08:03:30.825570Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ngrok configured successfully!\n",
      "üìù Don't have a token? Get one at: https://dashboard.ngrok.com/signup\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Configure Ngrok\n",
    "from pyngrok import ngrok, conf\n",
    "from huggingface_hub import login\n",
    "login(\"hf_token_here\")\n",
    "\n",
    "# ‚ö†Ô∏è REPLACE WITH YOUR NGROK TOKEN!\n",
    "NGROK_AUTH_TOKEN = \"ngrok_auth_token_here\"\n",
    "\n",
    "conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
    "print(\"‚úÖ Ngrok configured successfully!\")\n",
    "print(\"üìù Don't have a token? Get one at: https://dashboard.ngrok.com/signup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:31.004915Z",
     "iopub.status.busy": "2026-02-03T08:03:31.004467Z",
     "iopub.status.idle": "2026-02-03T08:03:31.019669Z",
     "shell.execute_reply": "2026-02-03T08:03:31.017282Z",
     "shell.execute_reply.started": "2026-02-03T08:03:31.004873Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import CrossEncoder, SentenceTransformer, util\n",
    "except ImportError:\n",
    "    print(\"Warning: sentence_transformers not available. Adaptive chunking methods may fail.\")\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"Warning: psutil not available. Resource usage metrics will be set to 0.0.\")\n",
    "    psutil = None\n",
    "try:\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    print(\"Warning: faiss not available. FAISS retrieval disabled.\")\n",
    "    faiss = None\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:31.025316Z",
     "iopub.status.busy": "2026-02-03T08:03:31.024870Z",
     "iopub.status.idle": "2026-02-03T08:03:31.051456Z",
     "shell.execute_reply": "2026-02-03T08:03:31.049895Z",
     "shell.execute_reply.started": "2026-02-03T08:03:31.025279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:31.055105Z",
     "iopub.status.busy": "2026-02-03T08:03:31.054695Z",
     "iopub.status.idle": "2026-02-03T08:03:31.225279Z",
     "shell.execute_reply": "2026-02-03T08:03:31.223097Z",
     "shell.execute_reply.started": "2026-02-03T08:03:31.055073Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG functions loaded! (Make sure you pasted your code above)\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Paste Your RAG Code Here\n",
    "# Copy all your chunking functions and OptimizedRAG class from your existing notebook\n",
    "# For example:\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub(r'[^\\x20-\\x7E]', '', text)\n",
    "    return text\n",
    "\n",
    "def read_pdf_from_bytes(pdf_bytes):\n",
    "    \"\"\"Read PDF from bytes\"\"\"\n",
    "    try:\n",
    "        pdf_file = BytesIO(pdf_bytes)\n",
    "        reader = PdfReader(pdf_file)\n",
    "        pages = []\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                pages.append(text)\n",
    "        return pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# TODO: Add your chunking methods here\n",
    "# - chunk_with_overlap\n",
    "def chunk_with_overlap(text, chunk_size=150, overlap=70, chunk_limit=150):\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += chunk_size - overlap\n",
    "        if start >= len(tokens):\n",
    "            break\n",
    "    return chunks[:chunk_limit] if chunk_limit else chunks\n",
    "\n",
    "#adaptive overlap chunking\n",
    "def adaptive_overlap_chunking(text_pages, chunk_size=300, min_overlap=30, max_overlap=80, chunk_limit=1000):\n",
    "    try:\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    except:\n",
    "        print(\"SentenceTransformer unavailable. Skipping adaptive chunking.\")\n",
    "        return []\n",
    "    if isinstance(text_pages, list):\n",
    "        text = ' '.join(str(page) for page in text_pages if page)\n",
    "    else:\n",
    "        text = str(text_pages)\n",
    "    text = clean_text(text)\n",
    "    paragraphs = re.split(r'\\n\\n+', text)\n",
    "    chunks = []\n",
    "    for para in paragraphs:\n",
    "        if not para.strip():\n",
    "            continue\n",
    "        sentences = re.split(r'(?<=[.!?]) +', para)\n",
    "        current = []\n",
    "        current_word_count = 0\n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if not sent:\n",
    "                continue\n",
    "            sent_word_count = len(sent.split())\n",
    "            if current_word_count + sent_word_count > chunk_size:\n",
    "                if current:\n",
    "                    chunk_text = \" \".join(current)\n",
    "                    chunks.append(chunk_text)\n",
    "                    if len(chunks) >= 2:\n",
    "                        try:\n",
    "                            prev_chunk = chunks[-2]\n",
    "                            curr_chunk = chunks[-1]\n",
    "                            embeddings = model.encode([prev_chunk, curr_chunk], \n",
    "                                                    convert_to_tensor=True, show_progress_bar=False)\n",
    "                            similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "                            overlap_words_count = int(min_overlap + (max_overlap - min_overlap) * (1 - similarity))\n",
    "                            overlap_words_count = max(min_overlap, min(overlap_words_count, max_overlap))\n",
    "                            overlap_words_count = min(overlap_words_count, len(chunk_text.split()))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Similarity calculation failed: {e}\")\n",
    "                            overlap_words_count = min_overlap\n",
    "                    else:\n",
    "                        overlap_words_count = min_overlap\n",
    "                    overlap_words = \" \".join(chunk_text.split()[-overlap_words_count:])\n",
    "                    current = [overlap_words, sent] if overlap_words else [sent]\n",
    "                    current_word_count = len(overlap_words.split()) + sent_word_count if overlap_words else sent_word_count\n",
    "                else:\n",
    "                    current = [sent]\n",
    "                    current_word_count = sent_word_count\n",
    "            else:\n",
    "                current.append(sent)\n",
    "                current_word_count += sent_word_count\n",
    "        if current:\n",
    "            chunks.append(\" \".join(current))\n",
    "    return chunks[:chunk_limit] if chunk_limit else chunks\n",
    "\n",
    "\n",
    "def improved_sentence_adaptive_chunking_wrt_sentence_density(\n",
    "    text_pages,\n",
    "    target_sentences=15,\n",
    "    min_overlap=2,\n",
    "    max_overlap=8,\n",
    "    alpha=1.25,\n",
    "    similarity_model='all-MiniLM-L6-v2',\n",
    "    verbose=False\n",
    "):\n",
    "    try:\n",
    "        model = SentenceTransformer(similarity_model)\n",
    "    except:\n",
    "        print(\"SentenceTransformer unavailable. Skipping improved_sentence_adaptive_wrt_sentence_density chunking.\")\n",
    "        return []\n",
    "    if isinstance(text_pages, list):\n",
    "        text = ' '.join(str(p) for p in text_pages if p)\n",
    "    else:\n",
    "        text = str(text_pages)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    merged_sentences = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        if len(sentences[i].split()) < 6 and i + 1 < len(sentences):\n",
    "            merged_sentences.append(sentences[i] + \" \" + sentences[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_sentences.append(sentences[i])\n",
    "            i += 1\n",
    "    sentences = merged_sentences\n",
    "    avg_words_per_sentence = np.mean([len(s.split()) for s in sentences])\n",
    "    dynamic_target_sentences = max(8, int(150 / avg_words_per_sentence))\n",
    "    words = [w.lower() for s in sentences for w in re.findall(r'\\b\\w+\\b', s)]\n",
    "    freq = Counter(words)\n",
    "    important_keywords = {w for w, c in freq.items() if c >= 3 and len(w) > 3}\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(sentences):\n",
    "        end = min(start + dynamic_target_sentences, len(sentences))\n",
    "        chunk = sentences[start:end]\n",
    "        if end < len(sentences):\n",
    "            for kw in important_keywords:\n",
    "                if (sentences[end-1].lower().endswith(kw) or sentences[end].lower().startswith(kw)) and end + 1 < len(sentences):\n",
    "                    end += 1\n",
    "        chunk = sentences[start:end]\n",
    "        chunks.append(' '.join(chunk))\n",
    "        if end >= len(sentences):\n",
    "            break\n",
    "        next_start = end\n",
    "        next_end = min(next_start + dynamic_target_sentences, len(sentences))\n",
    "        next_chunk_preview = sentences[next_start:next_end]\n",
    "        try:\n",
    "            embeddings = model.encode([' '.join(chunk), ' '.join(next_chunk_preview)], convert_to_tensor=True)\n",
    "            similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "            overlap_sentences = int(min_overlap + (max_overlap - min_overlap) * (1 - similarity) ** alpha)\n",
    "            overlap_sentences = max(min_overlap, min(overlap_sentences, max_overlap))\n",
    "        except:\n",
    "            overlap_sentences = min_overlap\n",
    "        if overlap_sentences >= end - start:\n",
    "            overlap_sentences = min_overlap\n",
    "        start = end - overlap_sentences\n",
    "    if len(chunks) > 1 and len(chunks[-1].split()) < (0.6 * dynamic_target_sentences * avg_words_per_sentence):\n",
    "        chunks[-2] += \" \" + chunks[-1]\n",
    "        chunks.pop()\n",
    "    final_chunks = []\n",
    "    seen = set()\n",
    "    for chunk in chunks:\n",
    "        lines = chunk.split('. ')\n",
    "        unique_lines = []\n",
    "        for l in lines:\n",
    "            if l not in seen:\n",
    "                unique_lines.append(l)\n",
    "                seen.add(l)\n",
    "        final_chunks.append('. '.join(unique_lines))\n",
    "    return final_chunks\n",
    "\n",
    "# - Gradient_chunking\n",
    "def Gradient_chunking(\n",
    "    text_pages,\n",
    "    target_sentences=15,\n",
    "    min_overlap=2,\n",
    "    max_overlap=8,\n",
    "    alpha=1.25,\n",
    "    similarity_model='all-MiniLM-L6-v2',\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced Sentence-Adaptive Chunking with CADS + AOSG:\n",
    "    ‚úÖ Content-Aware Dynamic Sizing (CADS): Adjusts chunk size based on entity/keyword density\n",
    "    ‚úÖ Adaptive Overlap Smoothing via Semantic Gradient (AOSG): Overlap based on similarity gradient\n",
    "    ‚úÖ Prevents out-of-index errors during keyword anchoring and overlaps\n",
    "    ‚úÖ Merges tiny sentences (<6 words) with next\n",
    "    ‚úÖ Handles small last chunk gracefully\n",
    "    ‚úÖ Reduces redundancy (removes duplicate sentences across chunks)\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚úÖ Combine text\n",
    "    if isinstance(text_pages, list):\n",
    "        text = ' '.join(str(p) for p in text_pages if p)\n",
    "    else:\n",
    "        text = str(text_pages)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # ‚úÖ Handle empty or invalid input\n",
    "    if not text:\n",
    "        if verbose:\n",
    "            print(\"[INFO] Empty or invalid input provided.\")\n",
    "        return []\n",
    "\n",
    "    # ‚úÖ Split into sentences\n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"[WARNING] Sentence tokenization failed: {e}. Falling back to regex-based splitting.\")\n",
    "        sentences = re.split(r'[.!?]+\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    # ‚úÖ Merge tiny sentences (<6 words) with next\n",
    "    merged_sentences = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        if len(sentences[i].split()) < 6 and i + 1 < len(sentences):\n",
    "            merged_sentences.append(sentences[i] + \" \" + sentences[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_sentences.append(sentences[i])\n",
    "            i += 1\n",
    "    sentences = merged_sentences\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] After merging tiny sentences: {len(sentences)} sentences\")\n",
    "\n",
    "    # ‚úÖ Calculate average sentence length density (words per sentence)\n",
    "    avg_words_per_sentence = np.mean([len(s.split()) for s in sentences])\n",
    "    base_target_sentences = max(8, int(150 / avg_words_per_sentence))  # Baseline ~150 words\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Avg words/sentence: {avg_words_per_sentence:.2f}, base target: {base_target_sentences} sentences\")\n",
    "\n",
    "    # ‚úÖ Extract important keywords for anchoring (frequency-based)\n",
    "    words = [w.lower() for s in sentences for w in re.findall(r'\\b\\w+\\b', s)]\n",
    "    freq = Counter(words)\n",
    "    important_keywords = {w for w, c in freq.items() if c >= 3 and len(w) > 3}\n",
    "\n",
    "    # ‚úÖ Compute content density using entities and keywords (CADS)\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"lemmatizer\"])  # Fast NER\n",
    "    content_density = []\n",
    "    for sent in sentences:\n",
    "        doc = nlp(sent)\n",
    "        entity_count = len([ent for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]])\n",
    "        keyword_count = sum(1 for w in re.findall(r'\\b\\w+\\b', sent.lower()) if w in important_keywords)\n",
    "        density = entity_count + keyword_count * 0.5  # Weight entities higher\n",
    "        content_density.append(density)\n",
    "    \n",
    "    # Normalize density to scale chunk sizes\n",
    "    max_density = max(content_density) if content_density else 1\n",
    "    chunk_sizes = [max(4, min(base_target_sentences * 2, int(base_target_sentences * (1 + d / max_density)))) \n",
    "                   for d in content_density]  # Scale between 0.5x and 2x base\n",
    "\n",
    "    model = SentenceTransformer(similarity_model)\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    i = 0\n",
    "\n",
    "    while start < len(sentences):\n",
    "        # ‚úÖ CADS: Use content-aware chunk size\n",
    "        chunk_size = chunk_sizes[min(i, len(chunk_sizes)-1)] if i < len(chunk_sizes) else base_target_sentences\n",
    "        end = min(start + chunk_size, len(sentences))\n",
    "        \n",
    "        # ‚úÖ Ensure keywords are not split between chunks\n",
    "        if end < len(sentences):\n",
    "            for kw in important_keywords:\n",
    "                if (sentences[end-1].lower().endswith(kw) or sentences[end].lower().startswith(kw)) and end + 1 < len(sentences):\n",
    "                    end += 1\n",
    "\n",
    "        chunk = sentences[start:end]\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "        if end >= len(sentences):\n",
    "            break\n",
    "\n",
    "        # ‚úÖ AOSG: Compute semantic gradient for overlap\n",
    "        next_start = end\n",
    "        next_end = min(next_start + base_target_sentences, len(sentences))\n",
    "        next_chunk_preview = sentences[next_start:next_end]\n",
    "\n",
    "        # Compute similarities over a window around the boundary\n",
    "        window_size = max_overlap * 2  # Look back/forward\n",
    "        sim_window_start = max(0, end - window_size)\n",
    "        sim_window_end = min(len(sentences) - 1, end + window_size)\n",
    "        similarities = []\n",
    "        for j in range(sim_window_start, sim_window_end):\n",
    "            if j + 1 < len(sentences):\n",
    "                sim = cos_sim(sentence_embeddings[j], sentence_embeddings[j + 1]).item()\n",
    "                similarities.append(sim)\n",
    "        \n",
    "        # Gradient: Rate of similarity change\n",
    "        gradients = [abs(similarities[j] - similarities[j - 1]) for j in range(1, len(similarities))]\n",
    "        if gradients:\n",
    "            avg_gradient = np.mean(gradients)\n",
    "            # Scale overlap by gradient: higher gradient (rapid change) -> larger overlap\n",
    "            #overlap_sentences = int(min_overlap + (max_overlap - min_overlap) * min(avg_gradient / 0.5, 1))\n",
    "            overlap_sentences = int(min_overlap + (max_overlap - min_overlap) * min(avg_gradient / 0.5, 1) )\n",
    "\n",
    "        else:\n",
    "            overlap_sentences = min_overlap\n",
    "\n",
    "        overlap_sentences = max(min_overlap, min(overlap_sentences, max_overlap))\n",
    "        \n",
    "        # ‚úÖ Prevent negative or too large overlaps\n",
    "        if overlap_sentences >= end - start:\n",
    "            overlap_sentences = min_overlap\n",
    "\n",
    "        start = end - overlap_sentences\n",
    "        i += 1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Chunk {len(chunks)}: {chunk_size} sentences (density: {content_density[start]:.2f}), overlap {overlap_sentences} (gradient: {avg_gradient:.3f} if defined)\")\n",
    "\n",
    "    # ‚úÖ Fix last small chunk (<60% of target)\n",
    "    if len(chunks) > 1 and len(chunks[-1].split()) < (0.6 * base_target_sentences * avg_words_per_sentence):\n",
    "        chunks[-2] += \" \" + chunks[-1]\n",
    "        chunks.pop()\n",
    "        if verbose:\n",
    "            print(\"[INFO] Last chunk merged (too small)\")\n",
    "\n",
    "    # ‚úÖ Post-process redundancy\n",
    "    final_chunks = []\n",
    "    seen = set()\n",
    "    for chunk in chunks:\n",
    "        lines = chunk.split('. ')\n",
    "        unique_lines = []\n",
    "        for l in lines:\n",
    "            if l not in seen:\n",
    "                unique_lines.append(l)\n",
    "                seen.add(l)\n",
    "        final_chunks.append('. '.join(unique_lines))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[DONE] Total chunks: {len(final_chunks)}\")\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "# - Gradient_chunking_final\n",
    "\n",
    "def Gradient_chunking_final(\n",
    "    text_pages,\n",
    "    target_sentences=15,\n",
    "    min_overlap=2,\n",
    "    max_overlap=8,\n",
    "    alpha=1.25,\n",
    "    similarity_model='all-MiniLM-L6-v2',\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced Sentence-Adaptive Chunking with Adaptive Overlap Smoothing via Semantic Gradient (AOSG):\n",
    "    ‚úÖ Uses average sentence length density to adjust chunk size dynamically\n",
    "    ‚úÖ Prevents out-of-index errors during keyword anchoring and overlaps\n",
    "    ‚úÖ Smart overlap based on semantic similarity gradient (novel: AOSG)\n",
    "    ‚úÖ Merges tiny sentences (<6 words) with next\n",
    "    ‚úÖ Handles small last chunk gracefully\n",
    "    ‚úÖ Reduces redundancy (removes duplicate sentences across chunks)\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚úÖ Combine text\n",
    "    if isinstance(text_pages, list):\n",
    "        text = ' '.join(str(p) for p in text_pages if p)\n",
    "    else:\n",
    "        text = str(text_pages)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # ‚úÖ Split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    # ‚úÖ Merge tiny sentences (<6 words) with next\n",
    "    merged_sentences = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        if len(sentences[i].split()) < 6 and i + 1 < len(sentences):\n",
    "            merged_sentences.append(sentences[i] + \" \" + sentences[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_sentences.append(sentences[i])\n",
    "            i += 1\n",
    "    sentences = merged_sentences\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[INFO] After merging tiny sentences: {len(sentences)} sentences\")\n",
    "\n",
    "    # ‚úÖ Calculate average sentence length density (words per sentence)\n",
    "    avg_words_per_sentence = np.mean([len(s.split()) for s in sentences])\n",
    "    dynamic_target_sentences = max(8, int(150 / avg_words_per_sentence))  # aim for ~150 words per chunk\n",
    "    if verbose:\n",
    "        print(f\"[INFO] Avg words/sentence: {avg_words_per_sentence:.2f}, dynamic target: {dynamic_target_sentences} sentences\")\n",
    "\n",
    "    # ‚úÖ Extract important keywords for anchoring (simple frequency-based)\n",
    "    words = [w.lower() for s in sentences for w in re.findall(r'\\b\\w+\\b', s)]\n",
    "    freq = Counter(words)\n",
    "    important_keywords = {w for w, c in freq.items() if c >= 3 and len(w) > 3}\n",
    "\n",
    "    model = SentenceTransformer(similarity_model)\n",
    "    \n",
    "    # ‚úÖ Precompute sentence embeddings for efficiency\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(sentences):\n",
    "        end = min(start + dynamic_target_sentences, len(sentences))\n",
    "        \n",
    "        # ‚úÖ Ensure keywords are not split between chunks (anchor adjustment)\n",
    "        if end < len(sentences):\n",
    "            for kw in important_keywords:\n",
    "                if (sentences[end-1].lower().endswith(kw) or sentences[end].lower().startswith(kw)) and end + 1 < len(sentences):\n",
    "                    end += 1\n",
    "\n",
    "        chunk = sentences[start:end]\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "        if end >= len(sentences):\n",
    "            break\n",
    "\n",
    "        # ‚úÖ Compute semantic gradient for smarter overlap (AOSG)\n",
    "        next_start = end\n",
    "        next_end = min(next_start + dynamic_target_sentences, len(sentences))\n",
    "        next_chunk_preview = sentences[next_start:next_end]\n",
    "\n",
    "        # Compute similarities over a window around the boundary\n",
    "        window_size = max_overlap * 2  # Look back/forward\n",
    "        sim_window_start = max(0, end - window_size)\n",
    "        sim_window_end = min(len(sentences) - 1, end + window_size)\n",
    "        similarities = []\n",
    "        for i in range(sim_window_start, sim_window_end):\n",
    "            if i + 1 < len(sentences):\n",
    "                sim = cos_sim(sentence_embeddings[i], sentence_embeddings[i + 1]).item()\n",
    "                similarities.append(sim)\n",
    "        \n",
    "        # Gradient: Rate of similarity change\n",
    "        gradients = [abs(similarities[i] - similarities[i - 1]) for i in range(1, len(similarities))]\n",
    "        if gradients:\n",
    "            avg_gradient = np.mean(gradients)\n",
    "            # Scale overlap by gradient: higher gradient (rapid change) -> larger overlap\n",
    "            overlap_sentences = int(min_overlap + (max_overlap - min_overlap) * min(avg_gradient / 0.5, 1))\n",
    "        else:\n",
    "            overlap_sentences = min_overlap\n",
    "\n",
    "        overlap_sentences = max(min_overlap, min(overlap_sentences, max_overlap))\n",
    "        \n",
    "        # ‚úÖ Prevent negative or too large overlaps\n",
    "        if overlap_sentences >= end - start:\n",
    "            overlap_sentences = min_overlap\n",
    "\n",
    "        start = end - overlap_sentences\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[INFO] Chunk {len(chunks)}: overlap {overlap_sentences} sentences (gradient: {avg_gradient:.3f} if defined)\")\n",
    "\n",
    "    # ‚úÖ Fix last small chunk (<60% of target)\n",
    "    if len(chunks) > 1 and len(chunks[-1].split()) < (0.6 * dynamic_target_sentences * avg_words_per_sentence):\n",
    "        chunks[-2] += \" \" + chunks[-1]\n",
    "        chunks.pop()\n",
    "        if verbose:\n",
    "            print(\"[INFO] Last chunk merged (too small)\")\n",
    "\n",
    "    # ‚úÖ Post-process redundancy (remove excessive duplicate sentences)\n",
    "    final_chunks = []\n",
    "    seen = set()\n",
    "    for chunk in chunks:\n",
    "        lines = chunk.split('. ')\n",
    "        unique_lines = []\n",
    "        for l in lines:\n",
    "            if l not in seen:\n",
    "                unique_lines.append(l)\n",
    "                seen.add(l)\n",
    "        final_chunks.append('. '.join(unique_lines))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[DONE] Total chunks: {len(final_chunks)}\")\n",
    "\n",
    "    return final_chunks\n",
    "\n",
    "\n",
    "# - etc.\n",
    "\n",
    "# TODO: Add your evaluate_chunk_quality function here\n",
    "def evaluate_chunk_quality(chunks, text):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(str(page) for page in text if page)\n",
    "    text = str(text).strip()\n",
    "    try:\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        chunk_embeddings = model.encode(chunks, convert_to_tensor=True, show_progress_bar=False)\n",
    "        text_embedding = model.encode([text], convert_to_tensor=True, show_progress_bar=False)[0]\n",
    "        coherence_scores = []\n",
    "        for i in range(len(chunks) - 1):\n",
    "            sim = util.cos_sim(chunk_embeddings[i], chunk_embeddings[i + 1]).item()\n",
    "            coherence_scores.append(sim)\n",
    "        avg_coherence = np.mean(coherence_scores) if coherence_scores else 0.0\n",
    "        similarities = util.cos_sim(chunk_embeddings, text_embedding.unsqueeze(0))\n",
    "        first_words = set(text.lower().split()[:10])\n",
    "        term_presence = sum(1 for chunk in chunks if any(word in chunk.lower() for word in first_words)) / len(chunks) if chunks else 0.0\n",
    "        avg_context_preservation = term_presence\n",
    "    except:\n",
    "        coherence_scores = []\n",
    "        for i in range(len(chunks) - 1):\n",
    "            set1 = set(chunks[i].lower().split())\n",
    "            set2 = set(chunks[i + 1].lower().split())\n",
    "            sim = len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0\n",
    "            coherence_scores.append(sim)\n",
    "        avg_coherence = np.mean(coherence_scores) if coherence_scores else 0.0\n",
    "        first_words = set(text.lower().split()[:10])\n",
    "        term_presence = sum(1 for chunk in chunks if any(word in chunk.lower() for word in first_words)) / len(chunks) if chunks else 0.0\n",
    "        avg_context_preservation = term_presence\n",
    "    chunk_lengths = [len(chunk.split()) for chunk in chunks]\n",
    "    avg_chunk_size = np.mean(chunk_lengths) if chunk_lengths else 0.0\n",
    "    std_chunk_size = np.std(chunk_lengths) if chunk_lengths else 0.0\n",
    "    size_consistency = std_chunk_size / avg_chunk_size if avg_chunk_size > 0 else 0\n",
    "    redundancy_score = 0\n",
    "    for i in range(len(chunks) - 1):\n",
    "        set1 = set(chunks[i].split())\n",
    "        set2 = set(chunks[i + 1].split())\n",
    "        if set1:\n",
    "            redundancy_score += len(set1 & set2) / len(set1)\n",
    "    redundancy_score /= (len(chunks) - 1) if len(chunks) > 1 else 0\n",
    "    original_words = set(text.split())\n",
    "    chunk_words = set(\" \".join(chunks).split())\n",
    "    coverage = len(chunk_words) / len(original_words) if original_words else 0\n",
    "    compression_ratio = len(\" \".join(chunks)) / len(text) if len(text) > 0 else 0\n",
    "    try:\n",
    "        sentences = [s.strip() for s in re.split(r'(?<=[.!?]) +', text) if s.strip()]\n",
    "        sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "        semantic_scores = []\n",
    "        for sent_emb in sentence_embeddings:\n",
    "            sim = util.cos_sim(sent_emb, chunk_embeddings).max().item()\n",
    "            semantic_scores.append(sim)\n",
    "        semantic_coverage = np.mean(semantic_scores) if semantic_scores else 0\n",
    "    except:\n",
    "        semantic_coverage = 0.0\n",
    "    def shannon_entropy(text_segment):\n",
    "        words = text_segment.split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        counts = Counter(words)\n",
    "        total = len(words)\n",
    "        entropy = -sum((count / total) * math.log2(count / total) for count in counts.values())\n",
    "        return entropy\n",
    "    info_density_scores = [shannon_entropy(chunk) for chunk in chunks]\n",
    "    avg_info_density = np.mean(info_density_scores) if info_density_scores else 0\n",
    "    weights = {\n",
    "        \"coherence\": 0.25,\n",
    "        \"context_preservation\": 0.25,\n",
    "        \"coverage\": 0.20,\n",
    "        \"semantic_coverage\": 0.20,\n",
    "        \"redundancy\": -0.10\n",
    "    }\n",
    "    weighted_score = (\n",
    "        avg_coherence * weights[\"coherence\"] +\n",
    "        avg_context_preservation * weights[\"context_preservation\"] +\n",
    "        coverage * weights[\"coverage\"] +\n",
    "        semantic_coverage * weights[\"semantic_coverage\"] +\n",
    "        redundancy_score * weights[\"redundancy\"]\n",
    "    )\n",
    "    return {\n",
    "        \"avg_coherence\": avg_coherence,\n",
    "        \"context_preservation\": avg_context_preservation,\n",
    "        \"avg_chunk_size\": avg_chunk_size,\n",
    "        \"size_consistency\": size_consistency,\n",
    "        \"redundancy\": redundancy_score,\n",
    "        \"coverage\": coverage,\n",
    "        \"compression_ratio\": compression_ratio,\n",
    "        \"semantic_coverage\": semantic_coverage,\n",
    "        \"avg_information_density\": avg_info_density,\n",
    "        \"weighted_score\": weighted_score\n",
    "    }\n",
    "\n",
    "\n",
    "# TODO: Add your OptimizedRAG class here\n",
    "class OptimizedRAG:\n",
    "    def __init__(self, documents, chunk_size=300, overlap=50, top_k=10, embed_model=\"all-MiniLM-L6-v2\", llm_model=\"google/flan-t5-large\", device=None,\n",
    "                 use_bm25=True, use_cosine=True, use_faiss=False, use_quantization=False, use_embedding_cache=True, use_batch_embedding=True,\n",
    "                 use_recursive_chunking=True, generation_mode=\"beam\", rerank_enabled=True, rerank_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "                 rerank_top_k=8, chunk_limit=1000, chunking_method=\"adaptive\"):\n",
    "        self.device = device or \"cpu\"\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents provided.\")\n",
    "        chunk_funcs = {\n",
    "            \"fixed\": lambda x: chunk_with_overlap(x, chunk_size, overlap, chunk_limit),\n",
    "            \"adaptive\": lambda x: adaptive_overlap_chunking(x, chunk_size, overlap, overlap * 3, chunk_limit),\n",
    "            \"improved_sentence_adaptive_chunking_wrt_sentence_density\": lambda x: improved_sentence_adaptive_chunking_wrt_sentence_density(x, target_sentences=chunk_size//20, min_overlap=max(1, overlap//20), max_overlap=max(2, overlap//10)),\n",
    "             \"Gradient_chunking\": lambda x: Gradient_chunking(x, target_sentences=chunk_size//20, min_overlap=max(1, overlap//20), max_overlap=max(2, overlap//10),alpha = 1.25),\n",
    "             \"Gradient_chunking_final\": lambda x: Gradient_chunking_final(x, target_sentences=chunk_size//20, min_overlap=max(1, overlap//20), max_overlap=max(2, overlap//10))\n",
    "\n",
    "\n",
    "        }\n",
    "        self.chunking_method = chunking_method\n",
    "        all_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = chunk_funcs[chunking_method](doc)\n",
    "            all_chunks.extend(chunks)\n",
    "        self.chunks = all_chunks\n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"No chunks generated from documents.\")\n",
    "        self.sentences = []\n",
    "        for chunk in self.chunks:\n",
    "            sentences = re.split(r'(?<=[.!?]) +', chunk)\n",
    "            cleaned = [re.sub(r\"http\\S+|www\\.\\S+\", \"\", s).strip() for s in sentences if len(s.strip()) > 20]\n",
    "            self.sentences.extend(cleaned)\n",
    "        self.use_bm25 = use_bm25\n",
    "        if use_bm25:\n",
    "            tokenized_chunks = [c.split() for c in self.chunks]\n",
    "            self.bm25 = BM25Okapi(tokenized_chunks)\n",
    "        self.use_cosine = use_cosine\n",
    "        self.use_faiss = use_faiss and faiss is not None\n",
    "        self.embed_models = {\n",
    "            \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"all-mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\"\n",
    "        }\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.embed_models.get(embed_model, \"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "            self.model = AutoModel.from_pretrained(self.embed_models.get(embed_model, \"sentence-transformers/all-MiniLM-L6-v2\")).to(self.device)\n",
    "        except:\n",
    "            print(\"Warning: Embedding model loading failed. Using dummy embeddings.\")\n",
    "            self.chunk_embeddings = np.random.rand(len(self.chunks), 384)\n",
    "        self.use_embedding_cache = use_embedding_cache\n",
    "        cache_file = f\"embeddings_{embed_model}.pkl\"\n",
    "        self.chunk_embeddings = None\n",
    "        if use_embedding_cache and os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, \"rb\") as f:\n",
    "                    self.chunk_embeddings = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading cache: {e}. Computing new embeddings.\")\n",
    "        if self.chunk_embeddings is None:\n",
    "            self.chunk_embeddings = self._encode_chunks(use_batch_embedding)\n",
    "            if use_embedding_cache:\n",
    "                try:\n",
    "                    with open(cache_file, \"wb\") as f:\n",
    "                        pickle.dump(self.chunk_embeddings, f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving cache: {e}\")\n",
    "        if self.use_faiss:\n",
    "            try:\n",
    "                dimension = self.chunk_embeddings.shape[1]\n",
    "                self.index = faiss.IndexFlatL2(dimension)\n",
    "                self.index.add(self.chunk_embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"FAISS index initialization failed: {e}. Disabling FAISS.\")\n",
    "                self.use_faiss = False\n",
    "                self.use_cosine = True\n",
    "        self.llm_models = {\n",
    "            \"google/flan-t5-large\": \"google/flan-t5-large\"\n",
    "        }\n",
    "        try:\n",
    "            self.llm_tokenizer = AutoTokenizer.from_pretrained(self.llm_models.get(llm_model, \"google/flan-t5-large\"))\n",
    "            self.llm_model = AutoModelForSeq2SeqLM.from_pretrained(self.llm_models.get(llm_model, \"google/flan-t5-large\")).to(self.device)\n",
    "        except:\n",
    "            print(\"Warning: LLM model loading failed. Generation may fail.\")\n",
    "        self.use_quantization = use_quantization\n",
    "        if use_quantization and self.device == \"cpu\":\n",
    "            try:\n",
    "                self.llm_model = torch.quantization.quantize_dynamic(self.llm_model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "            except Exception as e:\n",
    "                print(f\"Quantization failed: {e}. Proceeding without quantization.\")\n",
    "                self.use_quantization = False\n",
    "        self.generation_mode = generation_mode\n",
    "        self.top_k = top_k\n",
    "        self.rerank_enabled = rerank_enabled\n",
    "        if rerank_enabled:\n",
    "            self.rerank_model = rerank_model\n",
    "            self.rerank_top_k = rerank_top_k\n",
    "            try:\n",
    "                self.reranker = CrossEncoder(rerank_model)\n",
    "            except:\n",
    "                print(\"Warning: CrossEncoder unavailable. Disabling reranking.\")\n",
    "                self.rerank_enabled = False\n",
    "\n",
    "    def _encode_chunks(self, use_batch_embedding):\n",
    "        embeddings = []\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                if use_batch_embedding:\n",
    "                    batch_size = 16\n",
    "                    for i in range(0, len(self.chunks), batch_size):\n",
    "                        batch = self.chunks[i:i + batch_size]\n",
    "                        inputs = self.tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "                        outputs = self.model(**inputs)\n",
    "                        emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                        embeddings.append(emb)\n",
    "                    embeddings = np.vstack(embeddings)\n",
    "                else:\n",
    "                    for text in self.chunks:\n",
    "                        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "                        outputs = self.model(**inputs)\n",
    "                        emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "                        embeddings.append(emb)\n",
    "                    embeddings = np.array(embeddings)\n",
    "        except:\n",
    "            print(\"Embedding failed. Using dummy embeddings.\")\n",
    "            embeddings = np.random.rand(len(self.chunks), 384)\n",
    "        return embeddings\n",
    "\n",
    "    def retrieve(self, query):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                inputs = self.tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "                outputs = self.model(**inputs)\n",
    "                query_emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        except:\n",
    "            print(\"Query embedding failed. Using dummy query embedding.\")\n",
    "            query_emb = np.random.rand(384)\n",
    "        top_chunks = []\n",
    "        bm25_chunks = []\n",
    "        if self.use_bm25:\n",
    "            bm25_scores = self.bm25.get_scores(query.split())\n",
    "            bm25_indices = np.argsort(bm25_scores)[-self.top_k:][::-1]\n",
    "            bm25_chunks = [self.chunks[i] for i in bm25_indices]\n",
    "        if self.use_cosine:\n",
    "            sims = cosine_similarity(query_emb.reshape(1, -1), self.chunk_embeddings)[0]\n",
    "            top_indices = np.argsort(sims)[-self.top_k:][::-1]\n",
    "            top_chunks = [self.chunks[i] for i in top_indices]\n",
    "        if self.use_faiss:\n",
    "            try:\n",
    "                _, indices = self.index.search(query_emb.reshape(1, -1), self.top_k)\n",
    "                top_chunks = [self.chunks[i] for i in indices[0]]\n",
    "            except Exception as e:\n",
    "                print(f\"FAISS search failed: {e}. Falling back to cosine similarity.\")\n",
    "                self.use_faiss = False\n",
    "                self.use_cosine = True\n",
    "                sims = cosine_similarity(query_emb.reshape(1, -1), self.chunk_embeddings)[0]\n",
    "                top_indices = np.argsort(sims)[-self.top_k:][::-1]\n",
    "                top_chunks = [self.chunks[i] for i in top_indices]\n",
    "        if (self.use_cosine or self.use_faiss) and self.use_bm25:\n",
    "            merged = list(dict.fromkeys(top_chunks + bm25_chunks))\n",
    "            retrieved = merged[:self.top_k]\n",
    "        elif self.use_bm25:\n",
    "            retrieved = bm25_chunks\n",
    "        else:\n",
    "            retrieved = top_chunks\n",
    "        if self.rerank_enabled:\n",
    "            try:\n",
    "                pairs = [[query, chunk] for chunk in retrieved]\n",
    "                scores = self.reranker.predict(pairs)\n",
    "                reranked_indices = np.argsort(scores)[::-1][:self.rerank_top_k]\n",
    "                retrieved = [retrieved[i] for i in reranked_indices]\n",
    "                query_terms = set(query.lower().split())\n",
    "                retrieved = [chunk for chunk in retrieved if any(term in chunk.lower() for term in query_terms) and \n",
    "                             scores[reranked_indices[retrieved.index(chunk)]] > 0.25]\n",
    "            except:\n",
    "                print(\"Reranking failed. Using original retrieved chunks.\")\n",
    "        return retrieved\n",
    "\n",
    "    def clean_chunk(self, chunk):\n",
    "        cleaned = re.sub(r'[\\x00-\\x1f\\x7f-\\xff]', ' ', chunk)\n",
    "        cleaned = re.sub(r'\\\\n|\\\\uf0b7|\\s+', ' ', cleaned)\n",
    "        cleaned = re.sub(r'\\s+', ' ', ''.join(c for c in cleaned if not c.isspace() or c == ' '))\n",
    "        cleaned = cleaned.strip()\n",
    "        return cleaned\n",
    "\n",
    "    def generate_with_llm(self, query, context):\n",
    "        if not context:\n",
    "            context = \"No relevant context available.\"\n",
    "        max_context_length = 1024\n",
    "        query_terms = set(query.lower().split())\n",
    "        sentences = re.split(r'(?<=[.!?]) +', context)\n",
    "        filtered_sentences = sentences\n",
    "        if not filtered_sentences:\n",
    "            filtered_sentences = sentences\n",
    "        try:\n",
    "            combined_text = \" \".join(filtered_sentences)\n",
    "            sentences = [s.strip() for s in combined_text.split(\".\") if len(s) > 5]\n",
    "            summarized_context = \"\".join(sentences)[:max_context_length]\n",
    "            summarized_context = self.clean_chunk(summarized_context)\n",
    "        except Exception as e:\n",
    "            print(f\"Extractive summarization failed: {e}. Using original context.\")\n",
    "            summarized_context = \" \".join(filtered_sentences)[:max_context_length]\n",
    "        prompt = f\"\"\"\n",
    "        You are an expert assistant. Answer '{query}' in 2-3 concise sentences, focusing only on the topic. Explain its purpose and process briefly. Do not repeat the context word-for-word; synthesize a unique explanation. Limit to 50 words.\n",
    "        Context: {summarized_context}\n",
    "        Question: {query}\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        inputs = self.llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024).to(self.device)\n",
    "        kwargs = {\n",
    "            \"max_new_tokens\": 75,\n",
    "            \"min_length\": 10,\n",
    "            \"num_beams\": 5,\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"temperature\": 0.6,\n",
    "            \"top_k\": 40,\n",
    "            \"top_p\": 0.90,\n",
    "            \"do_sample\": True,\n",
    "            \"early_stopping\": True\n",
    "        }\n",
    "        try:\n",
    "            output_ids = self.llm_model.generate(**inputs, **kwargs)\n",
    "            response = self.llm_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            print(f\"LLM generation failed: {e}. Returning fallback response.\")\n",
    "            response = \"Error processing request. Please try again.\"\n",
    "        return response\n",
    "\n",
    "    def generate_answer(self, query, context_chunks):\n",
    "        if not context_chunks:\n",
    "            return \"No relevant content found.\"\n",
    "        cleaned_chunks = [self.clean_chunk(chunk) for chunk in context_chunks]\n",
    "        combined_context = \" \".join(cleaned_chunks)\n",
    "        try:\n",
    "            sentences = [s.strip() for s in combined_context.split(\".\") if len(s) > 5]\n",
    "            context = \"\".join(sentences)[:2048]\n",
    "        except Exception as e:\n",
    "            print(f\"Extractive summarization failed: {e}. Using cleaned chunks.\")\n",
    "            context = combined_context[:2048]\n",
    "        return self.generate_with_llm(query, context)\n",
    "\n",
    "    def evaluate(self, query, ground_truth=None):\n",
    "        start_time = time.time()\n",
    "        memory_usage = 0.0\n",
    "        cpu_usage = 0.0\n",
    "        if psutil:\n",
    "            process = psutil.Process()\n",
    "            memory_start = process.memory_info().rss / 1024 / 1024\n",
    "            cpu_times = []\n",
    "            def monitor_cpu():\n",
    "                try:\n",
    "                    cpu_times.append(process.cpu_percent(interval=0.1))\n",
    "                except:\n",
    "                    pass\n",
    "            import threading\n",
    "            cpu_monitor = threading.Thread(target=monitor_cpu)\n",
    "            cpu_monitor.daemon = True\n",
    "            cpu_monitor.start()\n",
    "        retrieved = self.retrieve(query)\n",
    "        response = self.generate_answer(query, retrieved)\n",
    "        latency = time.time() - start_time\n",
    "        if psutil:\n",
    "            memory_end = process.memory_info().rss / 1024 / 1024\n",
    "            memory_usage = max(memory_end - memory_start, 0.0)\n",
    "            cpu_usage = np.mean(cpu_times) if cpu_times else 0.0\n",
    "            cpu_monitor.join(timeout=0.1)\n",
    "        quality_metrics = evaluate_chunk_quality(self.chunks, \" \".join(self.chunks))\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"response\": response,\n",
    "            \"latency\": latency,\n",
    "            \"memory_usage\": memory_usage,\n",
    "            \"cpu_usage\": cpu_usage,\n",
    "            \"chunk_coherence\": quality_metrics[\"avg_coherence\"],\n",
    "            \"chunk_context_preservation\": quality_metrics[\"context_preservation\"],\n",
    "            \"avg_chunk_size\": quality_metrics[\"avg_chunk_size\"],\n",
    "            \"size_consistency\": quality_metrics[\"size_consistency\"],\n",
    "            \"redundancy\": quality_metrics[\"redundancy\"],\n",
    "            \"coverage\": quality_metrics[\"coverage\"],\n",
    "            \"compression_ratio\": quality_metrics[\"compression_ratio\"],\n",
    "            \"semantic_coverage\": quality_metrics[\"semantic_coverage\"],\n",
    "            \"avg_information_density\": quality_metrics[\"avg_information_density\"],\n",
    "            \"weighted_score\": quality_metrics[\"weighted_score\"]\n",
    "        }\n",
    "        if ground_truth:\n",
    "            scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "            scores = scorer.score(ground_truth, response)\n",
    "            result[\"rouge1\"] = scores['rouge1'].fmeasure\n",
    "            result[\"rougeL\"] = scores['rougeL'].fmeasure\n",
    "        return result\n",
    "\n",
    "\n",
    "print(\"‚úÖ RAG functions loaded! (Make sure you pasted your code above)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:31.227333Z",
     "iopub.status.busy": "2026-02-03T08:03:31.226959Z",
     "iopub.status.idle": "2026-02-03T08:03:31.291796Z",
     "shell.execute_reply": "2026-02-03T08:03:31.289747Z",
     "shell.execute_reply.started": "2026-02-03T08:03:31.227300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FastAPI app configured!\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: FastAPI Setup\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI(title=\"RAG Pipeline API\")\n",
    "\n",
    "# CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\n",
    "        \"http://localhost:3000\",              # your local React dev server\n",
    "        \"http://localhost:5173\",              # if using Vite instead of CRA\n",
    "        \"http://127.0.0.1:3000\",              # sometimes needed\n",
    "        \"*\"                                   # ‚Üê temporary wildcard for dev (less secure, but very convenient while testing)\n",
    "    ],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],                      # allow GET, POST, PUT, DELETE, OPTIONS, etc.\n",
    "    allow_headers=[\"*\"],                      # allow Content-Type, Authorization, etc.\n",
    ")\n",
    "\n",
    "\n",
    "# Models\n",
    "class RAGConfig(BaseModel):\n",
    "    chunkSize: int = 500\n",
    "    overlap: int = 50\n",
    "    method: str = \"gradient\"\n",
    "    useBM25: bool = True\n",
    "    useCosine: bool = True\n",
    "    useFaiss: bool = False\n",
    "    rerankEnabled: bool = True\n",
    "    topK: int = 4\n",
    "\n",
    "class ProcessRequest(BaseModel):\n",
    "    text: str\n",
    "    query: str\n",
    "    config: RAGConfig\n",
    "\n",
    "class ChunkData(BaseModel):\n",
    "    id: int\n",
    "    content: str\n",
    "\n",
    "class MetricsData(BaseModel):\n",
    "    num_chunks: int\n",
    "    weighted_score: float\n",
    "    latency: float\n",
    "    avg_coherence: float\n",
    "    context_preservation: float\n",
    "    avg_information_density: float\n",
    "    coverage: float\n",
    "    semantic_coverage: float\n",
    "    cpu_usage: float\n",
    "    memory_usage: float\n",
    "\n",
    "class RAGResponse(BaseModel):\n",
    "    response: str\n",
    "    chunks: List[ChunkData]\n",
    "    retrievedChunks: List[int]\n",
    "    metrics: MetricsData\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"RAG Pipeline API\", \"status\": \"healthy\"}\n",
    "\n",
    "@app.post(\"/upload_document\")\n",
    "async def upload_document(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        content = await file.read()\n",
    "        if not file.filename.endswith('.pdf'):\n",
    "            raise HTTPException(400, \"Only PDF files\")\n",
    "        \n",
    "        documents = read_pdf_from_bytes(content)\n",
    "        if not documents:\n",
    "            raise HTTPException(400, \"Could not extract text\")\n",
    "        \n",
    "        return {\n",
    "            \"filename\": file.filename,\n",
    "            \"extracted_text\": \" \".join(documents),\n",
    "            \"message\": \"Success\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(500, str(e))\n",
    "\n",
    "@app.post(\"/process\", response_model=RAGResponse)\n",
    "async def process_rag(request: ProcessRequest):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        process = psutil.Process()\n",
    "        memory_start = process.memory_info().rss / 1024 / 1024\n",
    "        \n",
    "        # Method mapping\n",
    "        method_map = {\n",
    "            \"fixed\": \"fixed\",\n",
    "            \"sentence_density\":\"improved_sentence_adaptive_chunking_wrt_sentence_density\",\n",
    "            \"adaptive\":\"adaptive\",\n",
    "            \"gradient\": \"Gradient_chunking\",\n",
    "            \"gradient_final\": \"Gradient_chunking_final\"\n",
    "        }\n",
    "        method = method_map.get(request.config.method)\n",
    "        if method is None:\n",
    "            raise ValueError(f\"Unknown chunking method: {request.config.method}\")\n",
    "        # Build config\n",
    "        rag_config = {\n",
    "            \"chunking_method\": method_map.get(request.config.method),\n",
    "            \"chunk_size\": request.config.chunkSize,\n",
    "            \"overlap\": request.config.overlap,\n",
    "            \"use_bm25\": request.config.useBM25,\n",
    "            \"use_cosine\": request.config.useCosine,\n",
    "            \"use_faiss\": request.config.useFaiss,\n",
    "            \"rerank_enabled\": request.config.rerankEnabled,\n",
    "            \"top_k\": request.config.topK,\n",
    "        }\n",
    "        \n",
    "        # Initialize RAG\n",
    "        rag = OptimizedRAG([request.text], **rag_config)\n",
    "        \n",
    "        # Retrieve and generate\n",
    "        retrieved = rag.retrieve(request.query)\n",
    "        response_text = rag.generate_answer(request.query, retrieved)\n",
    "        \n",
    "        # Metrics\n",
    "        latency = time.time() - start_time\n",
    "        memory_end = process.memory_info().rss / 1024 / 1024\n",
    "        memory_usage = max(memory_end - memory_start, 0.0)\n",
    "        cpu_usage = process.cpu_percent(interval=0.1)\n",
    "        \n",
    "        quality = evaluate_chunk_quality(rag.chunks, request.text)\n",
    "        \n",
    "        # Find retrieved indices\n",
    "        retrieved_indices = []\n",
    "        for rc in retrieved:\n",
    "            for idx, chunk in enumerate(rag.chunks):\n",
    "                if chunk == rc:\n",
    "                    retrieved_indices.append(idx)\n",
    "                    break\n",
    "        \n",
    "        return RAGResponse(\n",
    "            response=response_text,\n",
    "            chunks=[ChunkData(id=i, content=c) for i, c in enumerate(rag.chunks)],\n",
    "            retrievedChunks=retrieved_indices,\n",
    "            metrics=MetricsData(\n",
    "                num_chunks=len(rag.chunks),\n",
    "                weighted_score=quality[\"weighted_score\"],\n",
    "                latency=latency * 1000,\n",
    "                avg_coherence=quality[\"avg_coherence\"],\n",
    "                context_preservation=quality[\"context_preservation\"],\n",
    "                avg_information_density=quality[\"avg_information_density\"],\n",
    "                coverage=quality[\"coverage\"],\n",
    "                semantic_coverage=quality[\"semantic_coverage\"],\n",
    "                cpu_usage=cpu_usage,\n",
    "                memory_usage=memory_usage\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(500, str(e))\n",
    "\n",
    "print(\"‚úÖ FastAPI app configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:31.295688Z",
     "iopub.status.busy": "2026-02-03T08:03:31.294614Z",
     "iopub.status.idle": "2026-02-03T08:03:32.746946Z",
     "shell.execute_reply": "2026-02-03T08:03:32.744647Z",
     "shell.execute_reply.started": "2026-02-03T08:03:31.295636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
      "python3  55 root   63u  IPv4  30018      0t0  TCP *:8000 (LISTEN)\n"
     ]
    }
   ],
   "source": [
    "!lsof -i :8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:32.751307Z",
     "iopub.status.busy": "2026-02-03T08:03:32.750538Z",
     "iopub.status.idle": "2026-02-03T08:03:36.339074Z",
     "shell.execute_reply": "2026-02-03T08:03:36.337951Z",
     "shell.execute_reply.started": "2026-02-03T08:03:32.751259Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [55]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:9610 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ RAG PIPELINE API IS LIVE!\n",
      "================================================================================\n",
      "üì° Public URL: NgrokTunnel: \"https://0f3a59085f16.ngrok-free.app\" -> \"http://localhost:9610\"\n",
      "üìù Docs: NgrokTunnel: \"https://0f3a59085f16.ngrok-free.app\" -> \"http://localhost:9610\"/docs\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Copy this URL and update it in your React frontend!\n",
      "\n",
      "Update apiService.js:\n",
      "const API_BASE_URL = 'NgrokTunnel: \"https://0f3a59085f16.ngrok-free.app\" -> \"http://localhost:9610\"';\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Start Server with Ngrok\n",
    "import threading\n",
    "\n",
    "def run_server():\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9610, log_level=\"info\")\n",
    "\n",
    "# Start server\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "time.sleep(3)\n",
    "\n",
    "# Start ngrok\n",
    "public_url = ngrok.connect(9610)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ RAG PIPELINE API IS LIVE!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üì° Public URL: {public_url}\")\n",
    "print(f\"üìù Docs: {public_url}/docs\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "print(\"‚úÖ Copy this URL and update it in your React frontend!\")\n",
    "print(\"\")\n",
    "print(\"Update apiService.js:\")\n",
    "print(f\"const API_BASE_URL = '{public_url}';\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:36.341209Z",
     "iopub.status.busy": "2026-02-03T08:03:36.340752Z",
     "iopub.status.idle": "2026-02-03T08:03:36.351583Z",
     "shell.execute_reply": "2026-02-03T08:03:36.349407Z",
     "shell.execute_reply.started": "2026-02-03T08:03:36.341157Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Test failed: No connection adapters were found for 'NgrokTunnel: \"https://0f3a59085f16.ngrok-free.app\" -> \"http://localhost:9610\"/'\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Test API\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{public_url}/\")\n",
    "    print(\"‚úÖ API Test Successful!\")\n",
    "    print(f\"Response: {response.json()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T08:03:36.357338Z",
     "iopub.status.busy": "2026-02-03T08:03:36.356722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Server running at: NgrokTunnel: \"https://0f3a59085f16.ngrok-free.app\" -> \"http://localhost:9610\"\n",
      "üí° Keep this cell running to maintain connection\n",
      "‚ö†Ô∏è Free ngrok sessions timeout after 2 hours\n",
      "\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"OPTIONS /process HTTP/1.1\" 200 OK\n",
      "[INFO] After merging tiny sentences: 290 sentences\n",
      "[INFO] Avg words/sentence: 26.78, base target: 8 sentences\n",
      "[INFO] Chunk 1: 10 sentences (density: 2.00), overlap 3 (gradient: 0.194 if defined)\n",
      "[INFO] Chunk 2: 8 sentences (density: 8.00), overlap 3 (gradient: 0.214 if defined)\n",
      "[INFO] Chunk 3: 8 sentences (density: 3.50), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 4: 8 sentences (density: 7.50), overlap 2 (gradient: 0.144 if defined)\n",
      "[INFO] Chunk 5: 8 sentences (density: 3.50), overlap 2 (gradient: 0.150 if defined)\n",
      "[INFO] Chunk 6: 8 sentences (density: 13.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 7: 8 sentences (density: 14.00), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 8: 8 sentences (density: 4.00), overlap 2 (gradient: 0.152 if defined)\n",
      "[INFO] Chunk 9: 8 sentences (density: 4.50), overlap 2 (gradient: 0.146 if defined)\n",
      "[INFO] Chunk 10: 8 sentences (density: 4.00), overlap 3 (gradient: 0.193 if defined)\n",
      "[INFO] Chunk 11: 8 sentences (density: 14.00), overlap 3 (gradient: 0.208 if defined)\n",
      "[INFO] Chunk 12: 9 sentences (density: 16.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 13: 16 sentences (density: 13.00), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 14: 8 sentences (density: 16.00), overlap 3 (gradient: 0.180 if defined)\n",
      "[INFO] Chunk 15: 8 sentences (density: 2.00), overlap 3 (gradient: 0.169 if defined)\n",
      "[INFO] Chunk 16: 8 sentences (density: 9.00), overlap 2 (gradient: 0.139 if defined)\n",
      "[INFO] Chunk 17: 8 sentences (density: 11.50), overlap 3 (gradient: 0.177 if defined)\n",
      "[INFO] Chunk 18: 8 sentences (density: 8.50), overlap 2 (gradient: 0.132 if defined)\n",
      "[INFO] Chunk 19: 8 sentences (density: 11.00), overlap 2 (gradient: 0.136 if defined)\n",
      "[INFO] Chunk 20: 8 sentences (density: 3.50), overlap 2 (gradient: 0.137 if defined)\n",
      "[INFO] Chunk 21: 8 sentences (density: 11.50), overlap 3 (gradient: 0.181 if defined)\n",
      "[INFO] Chunk 22: 8 sentences (density: 12.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 23: 8 sentences (density: 4.00), overlap 3 (gradient: 0.170 if defined)\n",
      "[INFO] Chunk 24: 8 sentences (density: 17.50), overlap 2 (gradient: 0.125 if defined)\n",
      "[INFO] Chunk 25: 8 sentences (density: 1.50), overlap 2 (gradient: 0.116 if defined)\n",
      "[INFO] Chunk 26: 8 sentences (density: 12.50), overlap 2 (gradient: 0.129 if defined)\n",
      "[INFO] Chunk 27: 8 sentences (density: 6.00), overlap 2 (gradient: 0.139 if defined)\n",
      "[INFO] Chunk 28: 8 sentences (density: 12.50), overlap 2 (gradient: 0.157 if defined)\n",
      "[INFO] Chunk 29: 8 sentences (density: 2.50), overlap 2 (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 30: 8 sentences (density: 3.00), overlap 2 (gradient: 0.142 if defined)\n",
      "[INFO] Chunk 31: 8 sentences (density: 16.00), overlap 2 (gradient: 0.141 if defined)\n",
      "[INFO] Chunk 32: 8 sentences (density: 7.50), overlap 2 (gradient: 0.103 if defined)\n",
      "[INFO] Chunk 33: 8 sentences (density: 5.50), overlap 2 (gradient: 0.117 if defined)\n",
      "[INFO] Chunk 34: 8 sentences (density: 15.50), overlap 2 (gradient: 0.091 if defined)\n",
      "[INFO] Chunk 35: 8 sentences (density: 7.50), overlap 2 (gradient: 0.094 if defined)\n",
      "[INFO] Chunk 36: 8 sentences (density: 5.00), overlap 2 (gradient: 0.097 if defined)\n",
      "[INFO] Chunk 37: 8 sentences (density: 4.00), overlap 2 (gradient: 0.088 if defined)\n",
      "[INFO] Chunk 38: 8 sentences (density: 4.00), overlap 2 (gradient: 0.084 if defined)\n",
      "[INFO] Chunk 39: 8 sentences (density: 2.50), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 40: 9 sentences (density: 2.00), overlap 2 (gradient: 0.111 if defined)\n",
      "[INFO] Chunk 41: 8 sentences (density: 2.50), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 42: 8 sentences (density: 3.50), overlap 2 (gradient: 0.068 if defined)\n",
      "[INFO] Chunk 43: 8 sentences (density: 3.50), overlap 2 (gradient: 0.071 if defined)\n",
      "[INFO] Chunk 44: 8 sentences (density: 1.50), overlap 2 (gradient: 0.072 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 44\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 500 Internal Server Error\n",
      ".[INFO] After merging tiny sentences: 290 sentences\n",
      "[INFO] Avg words/sentence: 26.78, base target: 8 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/spacy/util.py:1751: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunk 1: 10 sentences (density: 2.00), overlap 3 (gradient: 0.194 if defined)\n",
      "[INFO] Chunk 2: 8 sentences (density: 8.00), overlap 3 (gradient: 0.214 if defined)\n",
      "[INFO] Chunk 3: 8 sentences (density: 3.50), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 4: 8 sentences (density: 7.50), overlap 2 (gradient: 0.144 if defined)\n",
      "[INFO] Chunk 5: 8 sentences (density: 3.50), overlap 2 (gradient: 0.150 if defined)\n",
      "[INFO] Chunk 6: 8 sentences (density: 13.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 7: 8 sentences (density: 14.00), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 8: 8 sentences (density: 4.00), overlap 2 (gradient: 0.152 if defined)\n",
      "[INFO] Chunk 9: 8 sentences (density: 4.50), overlap 2 (gradient: 0.146 if defined)\n",
      "[INFO] Chunk 10: 8 sentences (density: 4.00), overlap 3 (gradient: 0.193 if defined)\n",
      "[INFO] Chunk 11: 8 sentences (density: 14.00), overlap 3 (gradient: 0.208 if defined)\n",
      "[INFO] Chunk 12: 9 sentences (density: 16.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 13: 16 sentences (density: 13.00), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 14: 8 sentences (density: 16.00), overlap 3 (gradient: 0.180 if defined)\n",
      "[INFO] Chunk 15: 8 sentences (density: 2.00), overlap 3 (gradient: 0.169 if defined)\n",
      "[INFO] Chunk 16: 8 sentences (density: 9.00), overlap 2 (gradient: 0.139 if defined)\n",
      "[INFO] Chunk 17: 8 sentences (density: 11.50), overlap 3 (gradient: 0.177 if defined)\n",
      "[INFO] Chunk 18: 8 sentences (density: 8.50), overlap 2 (gradient: 0.132 if defined)\n",
      "[INFO] Chunk 19: 8 sentences (density: 11.00), overlap 2 (gradient: 0.136 if defined)\n",
      "[INFO] Chunk 20: 8 sentences (density: 3.50), overlap 2 (gradient: 0.137 if defined)\n",
      "[INFO] Chunk 21: 8 sentences (density: 11.50), overlap 3 (gradient: 0.181 if defined)\n",
      "[INFO] Chunk 22: 8 sentences (density: 12.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 23: 8 sentences (density: 4.00), overlap 3 (gradient: 0.170 if defined)\n",
      "[INFO] Chunk 24: 8 sentences (density: 17.50), overlap 2 (gradient: 0.125 if defined)\n",
      "[INFO] Chunk 25: 8 sentences (density: 1.50), overlap 2 (gradient: 0.116 if defined)\n",
      "[INFO] Chunk 26: 8 sentences (density: 12.50), overlap 2 (gradient: 0.129 if defined)\n",
      "[INFO] Chunk 27: 8 sentences (density: 6.00), overlap 2 (gradient: 0.139 if defined)\n",
      "[INFO] Chunk 28: 8 sentences (density: 12.50), overlap 2 (gradient: 0.157 if defined)\n",
      "[INFO] Chunk 29: 8 sentences (density: 2.50), overlap 2 (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 30: 8 sentences (density: 3.00), overlap 2 (gradient: 0.142 if defined)\n",
      "[INFO] Chunk 31: 8 sentences (density: 16.00), overlap 2 (gradient: 0.141 if defined)\n",
      "[INFO] Chunk 32: 8 sentences (density: 7.50), overlap 2 (gradient: 0.103 if defined)\n",
      "[INFO] Chunk 33: 8 sentences (density: 5.50), overlap 2 (gradient: 0.117 if defined)\n",
      "[INFO] Chunk 34: 8 sentences (density: 15.50), overlap 2 (gradient: 0.091 if defined)\n",
      "[INFO] Chunk 35: 8 sentences (density: 7.50), overlap 2 (gradient: 0.094 if defined)\n",
      "[INFO] Chunk 36: 8 sentences (density: 5.00), overlap 2 (gradient: 0.097 if defined)\n",
      "[INFO] Chunk 37: 8 sentences (density: 4.00), overlap 2 (gradient: 0.088 if defined)\n",
      "[INFO] Chunk 38: 8 sentences (density: 4.00), overlap 2 (gradient: 0.084 if defined)\n",
      "[INFO] Chunk 39: 8 sentences (density: 2.50), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 40: 9 sentences (density: 2.00), overlap 2 (gradient: 0.111 if defined)\n",
      "[INFO] Chunk 41: 8 sentences (density: 2.50), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 42: 8 sentences (density: 3.50), overlap 2 (gradient: 0.068 if defined)\n",
      "[INFO] Chunk 43: 8 sentences (density: 3.50), overlap 2 (gradient: 0.071 if defined)\n",
      "[INFO] Chunk 44: 8 sentences (density: 1.50), overlap 2 (gradient: 0.072 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 44\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      "[INFO] After merging tiny sentences: 290 sentences\n",
      "[INFO] Avg words/sentence: 26.78, dynamic target: 8 sentences\n",
      "[INFO] Chunk 1: overlap 3 sentences (gradient: 0.210 if defined)\n",
      "[INFO] Chunk 2: overlap 3 sentences (gradient: 0.193 if defined)\n",
      "[INFO] Chunk 3: overlap 3 sentences (gradient: 0.179 if defined)\n",
      "[INFO] Chunk 4: overlap 2 sentences (gradient: 0.143 if defined)\n",
      "[INFO] Chunk 5: overlap 2 sentences (gradient: 0.140 if defined)\n",
      "[INFO] Chunk 6: overlap 2 sentences (gradient: 0.162 if defined)\n",
      "[INFO] Chunk 7: overlap 3 sentences (gradient: 0.169 if defined)\n",
      "[INFO] Chunk 8: overlap 3 sentences (gradient: 0.180 if defined)\n",
      "[INFO] Chunk 9: overlap 2 sentences (gradient: 0.147 if defined)\n",
      "[INFO] Chunk 10: overlap 2 sentences (gradient: 0.152 if defined)\n",
      "[INFO] Chunk 11: overlap 3 sentences (gradient: 0.203 if defined)\n",
      "[INFO] Chunk 12: overlap 3 sentences (gradient: 0.187 if defined)\n",
      "[INFO] Chunk 13: overlap 2 sentences (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 14: overlap 2 sentences (gradient: 0.151 if defined)\n",
      "[INFO] Chunk 15: overlap 2 sentences (gradient: 0.166 if defined)\n",
      "[INFO] Chunk 16: overlap 3 sentences (gradient: 0.192 if defined)\n",
      "[INFO] Chunk 17: overlap 2 sentences (gradient: 0.156 if defined)\n",
      "[INFO] Chunk 18: overlap 2 sentences (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 19: overlap 2 sentences (gradient: 0.137 if defined)\n",
      "[INFO] Chunk 20: overlap 2 sentences (gradient: 0.116 if defined)\n",
      "[INFO] Chunk 21: overlap 2 sentences (gradient: 0.112 if defined)\n",
      "[INFO] Chunk 22: overlap 2 sentences (gradient: 0.165 if defined)\n",
      "[INFO] Chunk 23: overlap 3 sentences (gradient: 0.180 if defined)\n",
      "[INFO] Chunk 24: overlap 3 sentences (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 25: overlap 2 sentences (gradient: 0.125 if defined)\n",
      "[INFO] Chunk 26: overlap 2 sentences (gradient: 0.116 if defined)\n",
      "[INFO] Chunk 27: overlap 2 sentences (gradient: 0.129 if defined)\n",
      "[INFO] Chunk 28: overlap 2 sentences (gradient: 0.139 if defined)\n",
      "[INFO] Chunk 29: overlap 2 sentences (gradient: 0.157 if defined)\n",
      "[INFO] Chunk 30: overlap 2 sentences (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 31: overlap 2 sentences (gradient: 0.142 if defined)\n",
      "[INFO] Chunk 32: overlap 2 sentences (gradient: 0.141 if defined)\n",
      "[INFO] Chunk 33: overlap 2 sentences (gradient: 0.103 if defined)\n",
      "[INFO] Chunk 34: overlap 2 sentences (gradient: 0.117 if defined)\n",
      "[INFO] Chunk 35: overlap 2 sentences (gradient: 0.091 if defined)\n",
      "[INFO] Chunk 36: overlap 2 sentences (gradient: 0.094 if defined)\n",
      "[INFO] Chunk 37: overlap 2 sentences (gradient: 0.097 if defined)\n",
      "[INFO] Chunk 38: overlap 2 sentences (gradient: 0.088 if defined)\n",
      "[INFO] Chunk 39: overlap 2 sentences (gradient: 0.084 if defined)\n",
      "[INFO] Chunk 40: overlap 2 sentences (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 41: overlap 2 sentences (gradient: 0.109 if defined)\n",
      "[INFO] Chunk 42: overlap 2 sentences (gradient: 0.113 if defined)\n",
      "[INFO] Chunk 43: overlap 2 sentences (gradient: 0.073 if defined)\n",
      "[INFO] Chunk 44: overlap 2 sentences (gradient: 0.063 if defined)\n",
      "[INFO] Chunk 45: overlap 2 sentences (gradient: 0.062 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 45\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 500 Internal Server Error\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      ".[INFO] After merging tiny sentences: 290 sentences\n",
      "[INFO] Avg words/sentence: 26.78, dynamic target: 8 sentences\n",
      "[INFO] Chunk 1: overlap 3 sentences (gradient: 0.210 if defined)\n",
      "[INFO] Chunk 2: overlap 3 sentences (gradient: 0.193 if defined)\n",
      "[INFO] Chunk 3: overlap 3 sentences (gradient: 0.179 if defined)\n",
      "[INFO] Chunk 4: overlap 2 sentences (gradient: 0.143 if defined)\n",
      "[INFO] Chunk 5: overlap 2 sentences (gradient: 0.140 if defined)\n",
      "[INFO] Chunk 6: overlap 2 sentences (gradient: 0.162 if defined)\n",
      "[INFO] Chunk 7: overlap 3 sentences (gradient: 0.169 if defined)\n",
      "[INFO] Chunk 8: overlap 3 sentences (gradient: 0.180 if defined)\n",
      "[INFO] Chunk 9: overlap 2 sentences (gradient: 0.147 if defined)\n",
      "[INFO] Chunk 10: overlap 2 sentences (gradient: 0.152 if defined)\n",
      "[INFO] Chunk 11: overlap 3 sentences (gradient: 0.203 if defined)\n",
      "[INFO] Chunk 12: overlap 3 sentences (gradient: 0.187 if defined)\n",
      "[INFO] Chunk 13: overlap 2 sentences (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 14: overlap 2 sentences (gradient: 0.151 if defined)\n",
      "[INFO] Chunk 15: overlap 2 sentences (gradient: 0.166 if defined)\n",
      "[INFO] Chunk 16: overlap 3 sentences (gradient: 0.192 if defined)\n",
      "[INFO] Chunk 17: overlap 2 sentences (gradient: 0.156 if defined)\n",
      "[INFO] Chunk 18: overlap 2 sentences (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 19: overlap 2 sentences (gradient: 0.137 if defined)\n",
      "[INFO] Chunk 20: overlap 2 sentences (gradient: 0.116 if defined)\n",
      "[INFO] Chunk 21: overlap 2 sentences (gradient: 0.112 if defined)\n",
      "[INFO] Chunk 22: overlap 2 sentences (gradient: 0.165 if defined)\n",
      "[INFO] Chunk 23: overlap 3 sentences (gradient: 0.180 if defined)\n",
      "[INFO] Chunk 24: overlap 3 sentences (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 25: overlap 2 sentences (gradient: 0.125 if defined)\n",
      "[INFO] Chunk 26: overlap 2 sentences (gradient: 0.116 if defined)\n",
      "[INFO] Chunk 27: overlap 2 sentences (gradient: 0.129 if defined)\n",
      "[INFO] Chunk 28: overlap 2 sentences (gradient: 0.139 if defined)\n",
      "[INFO] Chunk 29: overlap 2 sentences (gradient: 0.157 if defined)\n",
      "[INFO] Chunk 30: overlap 2 sentences (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 31: overlap 2 sentences (gradient: 0.142 if defined)\n",
      "[INFO] Chunk 32: overlap 2 sentences (gradient: 0.141 if defined)\n",
      "[INFO] Chunk 33: overlap 2 sentences (gradient: 0.103 if defined)\n",
      "[INFO] Chunk 34: overlap 2 sentences (gradient: 0.117 if defined)\n",
      "[INFO] Chunk 35: overlap 2 sentences (gradient: 0.091 if defined)\n",
      "[INFO] Chunk 36: overlap 2 sentences (gradient: 0.094 if defined)\n",
      "[INFO] Chunk 37: overlap 2 sentences (gradient: 0.097 if defined)\n",
      "[INFO] Chunk 38: overlap 2 sentences (gradient: 0.088 if defined)\n",
      "[INFO] Chunk 39: overlap 2 sentences (gradient: 0.084 if defined)\n",
      "[INFO] Chunk 40: overlap 2 sentences (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 41: overlap 2 sentences (gradient: 0.109 if defined)\n",
      "[INFO] Chunk 42: overlap 2 sentences (gradient: 0.113 if defined)\n",
      "[INFO] Chunk 43: overlap 2 sentences (gradient: 0.073 if defined)\n",
      "[INFO] Chunk 44: overlap 2 sentences (gradient: 0.063 if defined)\n",
      "[INFO] Chunk 45: overlap 2 sentences (gradient: 0.062 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 45\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"OPTIONS /process HTTP/1.1\" 200 OK\n",
      "[INFO] After merging tiny sentences: 290 sentences\n",
      "[INFO] Avg words/sentence: 26.78, base target: 8 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/spacy/util.py:1751: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".[INFO] Chunk 1: 10 sentences (density: 2.00), overlap 3 (gradient: 0.194 if defined)\n",
      "[INFO] Chunk 2: 8 sentences (density: 8.00), overlap 3 (gradient: 0.214 if defined)\n",
      "[INFO] Chunk 3: 8 sentences (density: 3.50), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 4: 8 sentences (density: 7.50), overlap 2 (gradient: 0.144 if defined)\n",
      "[INFO] Chunk 5: 8 sentences (density: 3.50), overlap 2 (gradient: 0.150 if defined)\n",
      "[INFO] Chunk 6: 8 sentences (density: 13.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 7: 8 sentences (density: 14.00), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 8: 8 sentences (density: 4.00), overlap 2 (gradient: 0.152 if defined)\n",
      "[INFO] Chunk 9: 8 sentences (density: 4.50), overlap 2 (gradient: 0.146 if defined)\n",
      "[INFO] Chunk 10: 8 sentences (density: 4.00), overlap 3 (gradient: 0.193 if defined)\n",
      "[INFO] Chunk 11: 8 sentences (density: 14.00), overlap 3 (gradient: 0.208 if defined)\n",
      "[INFO] Chunk 12: 9 sentences (density: 16.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 13: 16 sentences (density: 13.00), overlap 3 (gradient: 0.167 if defined)\n",
      "[INFO] Chunk 14: 8 sentences (density: 16.00), overlap 3 (gradient: 0.180 if defined)\n",
      "[INFO] Chunk 15: 8 sentences (density: 2.00), overlap 3 (gradient: 0.169 if defined)\n",
      "[INFO] Chunk 16: 8 sentences (density: 9.00), overlap 2 (gradient: 0.139 if defined)\n",
      "[INFO] Chunk 17: 8 sentences (density: 11.50), overlap 3 (gradient: 0.177 if defined)\n",
      "[INFO] Chunk 18: 8 sentences (density: 8.50), overlap 2 (gradient: 0.132 if defined)\n",
      "[INFO] Chunk 19: 8 sentences (density: 11.00), overlap 2 (gradient: 0.136 if defined)\n",
      "[INFO] Chunk 20: 8 sentences (density: 3.50), overlap 2 (gradient: 0.137 if defined)\n",
      "[INFO] Chunk 21: 8 sentences (density: 11.50), overlap 3 (gradient: 0.181 if defined)\n",
      "[INFO] Chunk 22: 8 sentences (density: 12.00), overlap 2 (gradient: 0.163 if defined)\n",
      "[INFO] Chunk 23: 8 sentences (density: 4.00), overlap 3 (gradient: 0.170 if defined)\n",
      "[INFO] Chunk 24: 8 sentences (density: 17.50), overlap 2 (gradient: 0.125 if defined)\n",
      "[INFO] Chunk 25: 8 sentences (density: 1.50), overlap 2 (gradient: 0.116 if defined)\n",
      "[INFO] Chunk 26: 8 sentences (density: 12.50), overlap 2 (gradient: 0.129 if defined)\n",
      "[INFO] Chunk 27: 8 sentences (density: 6.00), overlap 2 (gradient: 0.139 if defined)\n",
      "[INFO] Chunk 28: 8 sentences (density: 12.50), overlap 2 (gradient: 0.157 if defined)\n",
      "[INFO] Chunk 29: 8 sentences (density: 2.50), overlap 2 (gradient: 0.160 if defined)\n",
      "[INFO] Chunk 30: 8 sentences (density: 3.00), overlap 2 (gradient: 0.142 if defined)\n",
      "[INFO] Chunk 31: 8 sentences (density: 16.00), overlap 2 (gradient: 0.141 if defined)\n",
      "[INFO] Chunk 32: 8 sentences (density: 7.50), overlap 2 (gradient: 0.103 if defined)\n",
      "[INFO] Chunk 33: 8 sentences (density: 5.50), overlap 2 (gradient: 0.117 if defined)\n",
      "[INFO] Chunk 34: 8 sentences (density: 15.50), overlap 2 (gradient: 0.091 if defined)\n",
      "[INFO] Chunk 35: 8 sentences (density: 7.50), overlap 2 (gradient: 0.094 if defined)\n",
      "[INFO] Chunk 36: 8 sentences (density: 5.00), overlap 2 (gradient: 0.097 if defined)\n",
      "[INFO] Chunk 37: 8 sentences (density: 4.00), overlap 2 (gradient: 0.088 if defined)\n",
      "[INFO] Chunk 38: 8 sentences (density: 4.00), overlap 2 (gradient: 0.084 if defined)\n",
      "[INFO] Chunk 39: 8 sentences (density: 2.50), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 40: 9 sentences (density: 2.00), overlap 2 (gradient: 0.111 if defined)\n",
      "[INFO] Chunk 41: 8 sentences (density: 2.50), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 42: 8 sentences (density: 3.50), overlap 2 (gradient: 0.068 if defined)\n",
      "[INFO] Chunk 43: 8 sentences (density: 3.50), overlap 2 (gradient: 0.071 if defined)\n",
      "[INFO] Chunk 44: 8 sentences (density: 1.50), overlap 2 (gradient: 0.072 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 44\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      ".......................................................................................................................INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"OPTIONS /process HTTP/1.1\" 200 OK\n",
      "[INFO] After merging tiny sentences: 93 sentences\n",
      "[INFO] Avg words/sentence: 16.01, base target: 9 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/spacy/util.py:1751: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunk 1: 17 sentences (density: 3.00), overlap 2 (gradient: 0.140 if defined)\n",
      "[INFO] Chunk 2: 11 sentences (density: 4.50), overlap 3 (gradient: 0.172 if defined)\n",
      "[INFO] Chunk 3: 12 sentences (density: 2.50), overlap 3 (gradient: 0.186 if defined)\n",
      "[INFO] Chunk 4: 12 sentences (density: 4.50), overlap 3 (gradient: 0.210 if defined)\n",
      "[INFO] Chunk 5: 16 sentences (density: 9.50), overlap 3 (gradient: 0.168 if defined)\n",
      "[INFO] Chunk 6: 12 sentences (density: 8.00), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 7: 15 sentences (density: 1.00), overlap 2 (gradient: 0.064 if defined)\n",
      "[INFO] Chunk 8: 10 sentences (density: 0.50), overlap 2 (gradient: 0.071 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 8\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      "..........INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"OPTIONS /process HTTP/1.1\" 200 OK\n",
      "[INFO] After merging tiny sentences: 93 sentences\n",
      "[INFO] Avg words/sentence: 16.01, base target: 9 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/spacy/util.py:1751: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunk 1: 17 sentences (density: 3.00), overlap 2 (gradient: 0.140 if defined)\n",
      "[INFO] Chunk 2: 11 sentences (density: 4.50), overlap 3 (gradient: 0.172 if defined)\n",
      "[INFO] Chunk 3: 12 sentences (density: 2.50), overlap 3 (gradient: 0.186 if defined)\n",
      "[INFO] Chunk 4: 12 sentences (density: 4.50), overlap 3 (gradient: 0.210 if defined)\n",
      "[INFO] Chunk 5: 16 sentences (density: 9.50), overlap 3 (gradient: 0.168 if defined)\n",
      "[INFO] Chunk 6: 12 sentences (density: 8.00), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 7: 15 sentences (density: 1.00), overlap 2 (gradient: 0.064 if defined)\n",
      "[INFO] Chunk 8: 10 sentences (density: 0.50), overlap 2 (gradient: 0.071 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 8\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      ".........INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      "[INFO] After merging tiny sentences: 93 sentences\n",
      "[INFO] Avg words/sentence: 16.01, base target: 9 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/spacy/util.py:1751: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunk 1: 17 sentences (density: 3.00), overlap 2 (gradient: 0.140 if defined)\n",
      "[INFO] Chunk 2: 11 sentences (density: 4.50), overlap 3 (gradient: 0.172 if defined)\n",
      "[INFO] Chunk 3: 12 sentences (density: 2.50), overlap 3 (gradient: 0.186 if defined)\n",
      "[INFO] Chunk 4: 12 sentences (density: 4.50), overlap 3 (gradient: 0.210 if defined)\n",
      "[INFO] Chunk 5: 16 sentences (density: 9.50), overlap 3 (gradient: 0.168 if defined)\n",
      "[INFO] Chunk 6: 12 sentences (density: 8.00), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 7: 15 sentences (density: 1.00), overlap 2 (gradient: 0.064 if defined)\n",
      "[INFO] Chunk 8: 10 sentences (density: 0.50), overlap 2 (gradient: 0.071 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 8\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      "......INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"OPTIONS /process HTTP/1.1\" 200 OK\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 500 Internal Server Error\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 500 Internal Server Error\n",
      "[INFO] After merging tiny sentences: 93 sentences\n",
      "[INFO] Avg words/sentence: 16.01, base target: 9 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/spacy/util.py:1751: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Chunk 1: 17 sentences (density: 3.00), overlap 2 (gradient: 0.140 if defined)\n",
      "[INFO] Chunk 2: 11 sentences (density: 4.50), overlap 3 (gradient: 0.172 if defined)\n",
      "[INFO] Chunk 3: 12 sentences (density: 2.50), overlap 3 (gradient: 0.186 if defined)\n",
      "[INFO] Chunk 4: 12 sentences (density: 4.50), overlap 3 (gradient: 0.210 if defined)\n",
      "[INFO] Chunk 5: 16 sentences (density: 9.50), overlap 3 (gradient: 0.168 if defined)\n",
      "[INFO] Chunk 6: 12 sentences (density: 8.00), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 7: 15 sentences (density: 1.00), overlap 2 (gradient: 0.064 if defined)\n",
      "[INFO] Chunk 8: 10 sentences (density: 0.50), overlap 2 (gradient: 0.071 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 8\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      "...INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 500 Internal Server Error\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 500 Internal Server Error\n",
      "..INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 500 Internal Server Error\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 500 Internal Server Error\n",
      ".INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      "..INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      "............INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      ".....INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /upload_document HTTP/1.1\" 200 OK\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"OPTIONS /process HTTP/1.1\" 200 OK\n",
      "[INFO] After merging tiny sentences: 93 sentences\n",
      "[INFO] Avg words/sentence: 16.01, base target: 9 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/spacy/util.py:1751: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".[INFO] Chunk 1: 17 sentences (density: 3.00), overlap 2 (gradient: 0.140 if defined)\n",
      "[INFO] Chunk 2: 11 sentences (density: 4.50), overlap 3 (gradient: 0.172 if defined)\n",
      "[INFO] Chunk 3: 12 sentences (density: 2.50), overlap 3 (gradient: 0.186 if defined)\n",
      "[INFO] Chunk 4: 12 sentences (density: 4.50), overlap 3 (gradient: 0.210 if defined)\n",
      "[INFO] Chunk 5: 16 sentences (density: 9.50), overlap 3 (gradient: 0.168 if defined)\n",
      "[INFO] Chunk 6: 12 sentences (density: 8.00), overlap 2 (gradient: 0.102 if defined)\n",
      "[INFO] Chunk 7: 15 sentences (density: 1.00), overlap 2 (gradient: 0.064 if defined)\n",
      "[INFO] Chunk 8: 10 sentences (density: 0.50), overlap 2 (gradient: 0.071 if defined)\n",
      "[INFO] Last chunk merged (too small)\n",
      "[DONE] Total chunks: 8\n",
      "INFO:     2406:b400:66:211c:69ca:675d:cf14:2491:0 - \"POST /process HTTP/1.1\" 200 OK\n",
      ".............."
     ]
    }
   ],
   "source": [
    "# CELL 8: Keep Alive (Keep this running!)\n",
    "print(f\"üîÑ Server running at: {public_url}\")\n",
    "print(\"üí° Keep this cell running to maintain connection\")\n",
    "print(\"‚ö†Ô∏è Free ngrok sessions timeout after 2 hours\")\n",
    "print(\"\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(60)\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Server stopped\")\n",
    "    ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
